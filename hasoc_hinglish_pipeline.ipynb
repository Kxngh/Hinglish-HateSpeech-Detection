{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd2e027",
   "metadata": {},
   "source": [
    "# HASOC2019 Hinglish Hate Speech Classification Pipeline\n",
    "\n",
    "This notebook covers:\n",
    "1. Library installation\n",
    "2. Data loading\n",
    "3. Preprocessing (transliteration, emoji mapping, text normalization)\n",
    "4. Feature engineering (TF-IDF, char n-grams, numeric, lexicon, code-mix)\n",
    "5. Classical ML baseline (LogReg, SVM, RF)\n",
    "6. Transformer fine-tuning (mBERT / XLM-R)\n",
    "7. Ensembling\n",
    "8. Evaluation & saving models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install necessary libraries\n",
    "!pip install emoji aksharamukha sklearn imblearn transformers torch langdetect indic-nlp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa80b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Imports\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import emoji\n",
    "from aksharamukha import transliterate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load HASOC2019 Hinglish dataset\n",
    "# Adjust the path to where your CSV/TSV file is located\n",
    "df = pd.read_csv('hasoc2019_hinglish_train.csv')\n",
    "# Expect columns: 'text', 'label'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preprocessing functions\n",
    "\n",
    "# a) Emoji mapping\n",
    "def map_emojis(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "# b) Transliterate Roman‐Hindi to Devanagari (optional)\n",
    "def transliterate_hinglish(text):\n",
    "    try:\n",
    "        return transliterate.process('IAST', 'Devanagari', text)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# c) Normalize text\n",
    "def normalize_text(text):\n",
    "    text = map_emojis(text)\n",
    "    text = transliterate_hinglish(text)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)       # remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)                     # remove mentions\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)                # punct -> space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['text'].apply(normalize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd2178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature engineering\n",
    "\n",
    "# a) Numeric features\n",
    "def extract_numeric_feats(text):\n",
    "    tokens = text.split()\n",
    "    num_words = len(tokens)\n",
    "    uppercase_ratio = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "    excls = text.count('!')\n",
    "    ques = text.count('?')\n",
    "    # code-mix ratio\n",
    "    lang_tags = [detect(tok) for tok in tokens if len(tok) > 0]\n",
    "    hinglish_ratio = lang_tags.count('hi') / max(len(lang_tags), 1)\n",
    "    return [num_words, uppercase_ratio, excls, ques, hinglish_ratio]\n",
    "\n",
    "numeric_feats = np.array([extract_numeric_feats(t) for t in df['clean_text']])\n",
    "\n",
    "# b) TF-IDF word n-grams\n",
    "tfidf_word = TfidfVectorizer(ngram_range=(1,3), min_df=3, max_df=0.9, max_features=20000)\n",
    "X_tfidf_word = tfidf_word.fit_transform(df['clean_text'])\n",
    "\n",
    "# c) TF-IDF char n-grams\n",
    "tfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(3,6), max_features=20000)\n",
    "X_tfidf_char = tfidf_char.fit_transform(df['clean_text'])\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([X_tfidf_word, X_tfidf_char, numeric_feats])\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d29792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train/test split and oversampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.9)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff14082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Classical ML baseline\n",
    "models = {\n",
    "    'LogReg': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    'SVM': LinearSVC(class_weight='balanced', max_iter=5000),\n",
    "    'RF': RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"{name} F1: {f1_score(y_test, preds, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394bb167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Transformer fine-tuning (e.g. XLM-RoBERTa)\n",
    "checkpoint = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Tokenization\n",
    "train_enc = tokenizer(df['clean_text'].tolist(), truncation=True, padding=True)\n",
    "# Build dataset object\n",
    "class HateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = HateDataset(train_enc, df['label'])\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir='./transformer_out', num_train_epochs=3,\n",
    "    per_device_train_batch_size=16, evaluation_strategy='epoch',\n",
    "    save_total_limit=1, load_best_model_at_end=True\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(y.unique()))\n",
    "trainer = Trainer(model=model, args=args, train_dataset=train_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c157c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Ensemble classical + transformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# Calibrate SVM for probabilities\n",
    "svm_cal = CalibratedClassifierCV(models['SVM'], cv=3).fit(X_train_res, y_train_res)\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', models['LogReg']),\n",
    "        ('svm', svm_cal),\n",
    "        ('rf', models['RF'])\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "ensemble.fit(X_train_res, y_train_res)\n",
    "ensemble_preds = ensemble.predict(X_test)\n",
    "print(f\"Ensemble F1: {f1_score(y_test, ensemble_preds, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fea613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Save pipelines and models\n",
    "import joblib\n",
    "joblib.dump({\n",
    "    'tfidf_word': tfidf_word,\n",
    "    'tfidf_char': tfidf_char,\n",
    "    'numeric_feats_fn': extract_numeric_feats,\n",
    "    'classical_ensemble': ensemble\n",
    "}, 'hasoc_hinglish_classical.pkl\")\n",
    "# Save transformer\n",
    "model.save_pretrained('hasoc_transformer/')\n",
    "tokenizer.save_pretrained('hasoc_transformer/')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
