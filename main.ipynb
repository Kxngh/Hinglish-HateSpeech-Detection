{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ba3499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in ./.venv/lib64/python3.13/site-packages (0.1.73)\n",
      "Requirement already satisfied: pandas in ./.venv/lib64/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in ./.venv/lib64/python3.13/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib64/python3.13/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib64/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib64/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib64/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib64/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: anyascii in ./.venv/lib64/python3.13/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in ./.venv/lib64/python3.13/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "                                                text  hate\n",
      "0   Are bhajans played daily.??  5 times a day..?? ðŸ¤”     1\n",
      "1  @zainabsikander Perfectly said.... 100% agreed...     1\n",
      "2  Tats sad no mother thinks her baby as puppy bu...     1\n",
      "3  Speechs wont work if ill thoughts still around...     1\n",
      "4  @timesofindia Mamta is doing wrong. But where ...     0\n",
      "hate\n",
      "0    9507\n",
      "1    9497\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "%pip install contractions pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "df = pd.read_csv(\"combined_dataset.csv\", encoding='utf-8')  # First dataset\n",
    "\n",
    "\n",
    "# Print sample data\n",
    "print(df.head())\n",
    "print(df['hate'].value_counts())  # Check label distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e720ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in ./.venv/lib64/python3.13/site-packages (0.1.73)\n",
      "Requirement already satisfied: nltk in ./.venv/lib64/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib64/python3.13/site-packages (2.2.4)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib64/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in ./.venv/lib64/python3.13/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: click in ./.venv/lib64/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./.venv/lib64/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib64/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib64/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib64/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib64/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: anyascii in ./.venv/lib64/python3.13/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in ./.venv/lib64/python3.13/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install contractions nltk numpy scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6864c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import contractions\n",
    "\n",
    "def clean_text(text):\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Remove @mentions and hashtags\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    # Remove non-ASCII characters (e.g., emojis)\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # Remove extra whitespace and non-alphabetic characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Load your dataset (ensure the CSV has a 'text' column)\n",
    "df = pd.read_csv(\"combined_dataset.csv\")\n",
    "\n",
    "# Apply cleaning function to each text entry\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Save the cleaned texts into a corpus file, one sentence per line\n",
    "with open(\"hinglish_corpus.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for text in df['cleaned_text']:\n",
    "        if text.strip():  # Write non-empty lines only\n",
    "            f.write(text.strip() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29adb875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in ./.venv/lib64/python3.13/site-packages (0.9.3)\n",
      "Requirement already satisfied: pybind11>=2.2 in ./.venv/lib64/python3.13/site-packages (from fasttext) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./.venv/lib64/python3.13/site-packages (from fasttext) (78.1.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib64/python3.13/site-packages (from fasttext) (2.2.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  6445\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  123569 lr:  0.000000 avg.loss:  2.463794 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%pip install fasttext\n",
    "import fasttext\n",
    "\n",
    "model = fasttext.train_unsupervised('hinglish_corpus.txt', model='skipgram')\n",
    "model.save_model(\"hinglish_embeddings.bin\")\n",
    "model = fasttext.load_model(\"hinglish_embeddings.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c82cb02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016be3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (Îµ): 0.1\n",
      "Accuracy: 0.8650355169692187\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Not Hate       0.85      0.89      0.87      1902\n",
      "        Hate       0.88      0.84      0.86      1899\n",
      "\n",
      "    accuracy                           0.87      3801\n",
      "   macro avg       0.87      0.87      0.86      3801\n",
      "weighted avg       0.87      0.87      0.86      3801\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1692  210]\n",
      " [ 303 1596]]\n",
      "Testing time per statement: 0.1673 Âµs\n",
      "10-fold CV Accuracy: 0.8718\n",
      "95% CI: (0.8565, 0.8871)\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#data preprocessing\n",
    "def clean_text(text):\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Feature extraction: BoW(uni to tri)(binary)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,4),binary=True)\n",
    "X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "y = df['hate']\n",
    "\n",
    "# Feature selection using Mutual information classification\n",
    "selector = SelectKBest(score_func=chi2, k=40000)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "# Stratified train-test split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in sss.split(X_selected, y):\n",
    "    X_train, X_test = X_selected[train_idx], X_selected[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "\n",
    "# Model training with GridSearch for smoothing (epsilon / alpha)\n",
    "parameters = {'alpha': list(np.arange(0.1, 4, 0.05))}\n",
    "nb = MultinomialNB()\n",
    "clf = GridSearchCV(nb, parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "best_model = clf.best_estimator_\n",
    "\n",
    "# Evaluation\n",
    "start = time.time()\n",
    "y_pred = best_model.predict(X_test)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Best alpha (Îµ):\", clf.best_params_['alpha'])\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Not Hate', 'Hate']))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(f\"Testing time per statement: {(end - start) / len(y_test) * 1e6:.4f} Âµs\")\n",
    "\n",
    "# 10-fold cross-validation\n",
    "cv_scores = cross_val_score(best_model, X_selected, y, cv=10)\n",
    "print(\"10-fold CV Accuracy: %.4f\" % cv_scores.mean())\n",
    "print(\"95%% CI: (%.4f, %.4f)\" % (cv_scores.mean() - 2 * cv_scores.std(), cv_scores.mean() + 2 * cv_scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb635da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Î±: 2\n",
      "\n",
      "Test-set performance:\n",
      "Accuracy:  0.7077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Hate     0.7132    0.6956    0.7043      1902\n",
      "        Hate     0.7025    0.7199    0.7111      1899\n",
      "\n",
      "    accuracy                         0.7077      3801\n",
      "   macro avg     0.7078    0.7077    0.7077      3801\n",
      "weighted avg     0.7078    0.7077    0.7077      3801\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1323  579]\n",
      " [ 532 1367]]\n"
     ]
    }
   ],
   "source": [
    "import re, time, contractions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# â”€â”€ 1. Download NLTK resources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "\n",
    "# â”€â”€ 2. Cleaning function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def clean_text_series(texts):\n",
    "    cleaned = []\n",
    "    for doc in texts:\n",
    "        txt = contractions.fix(str(doc))                             # expand contractions\n",
    "        txt = re.sub(r'@\\w+|http\\S+|www.\\S+', ' ', txt)               # strip mentions/URLs\n",
    "        txt = re.sub(r\"[^a-zA-Z\\s]\", ' ', txt).lower()               # keep letters + lowercase\n",
    "        toks = [lemmatizer.lemmatize(tok) for tok in txt.split()\n",
    "                if tok not in STOP_WORDS]                           # lemmatize & drop stopwords\n",
    "        cleaned.append(' '.join(toks))\n",
    "    return np.array(cleaned)\n",
    "\n",
    "cleaner = FunctionTransformer(clean_text_series, validate=False)\n",
    "\n",
    "# â”€â”€ 3. Prepare data & split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df['cleaned_text'] = clean_text_series(df['text'])\n",
    "X_raw = df['cleaned_text']\n",
    "y     = df['hate'].values\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "train_idx, test_idx = next(sss.split(X_raw, y))\n",
    "X_train, X_test = X_raw[train_idx], X_raw[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "# â”€â”€ 4. Build pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pipeline = Pipeline([\n",
    "    ('clean', cleaner),\n",
    "    ('vect', CountVectorizer(ngram_range=(1,4), binary=True)),\n",
    "    ('select', SelectKBest(score_func=mutual_info_classif, k=40000)),\n",
    "    ('clf',   MultinomialNB())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__alpha': [0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "# â”€â”€ 5. Nested CV & hyperparameter search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "search   = GridSearchCV(pipeline, param_grid,\n",
    "                        scoring='f1_macro',\n",
    "                        cv=cv_inner,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Î±:\", search.best_params_['clf__alpha'])\n",
    "\n",
    "# â”€â”€ 6. Final evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_model = search.best_estimator_\n",
    "y_pred     = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nTest-set performance:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred,\n",
    "      target_names=['Not Hate','Hate'], digits=4))               # :contentReference[oaicite:9]{index=9}\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9e1851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [num_words, uppercase_ratio, exclamation_count, question_count]\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreprocessing text data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].apply(advanced_clean_text)\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Extract additional text features\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExtracting text features...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Enhanced preprocessing function with sentiment features\n",
    "def advanced_clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Fix contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Keep hashtag content\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    \n",
    "    # Convert to ASCII\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Keep only letters and spaces, convert to lowercase\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize using NLTK for better handling\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize and remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Additional feature extraction: text length, uppercase ratio, etc.\n",
    "def extract_text_features(text):\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return [0, 0, 0, 0]\n",
    "    \n",
    "    # Original text before cleaning for some features\n",
    "    original_text = text\n",
    "    \n",
    "    # Number of words\n",
    "    num_words = len(original_text.split())\n",
    "    \n",
    "    # Ratio of uppercase letters\n",
    "    if len(original_text) > 0:\n",
    "        uppercase_ratio = sum(1 for c in original_text if c.isupper()) / len(original_text)\n",
    "    else:\n",
    "        uppercase_ratio = 0\n",
    "    \n",
    "    # Number of exclamation marks\n",
    "    exclamation_count = original_text.count('!')\n",
    "    \n",
    "    # Number of question marks\n",
    "    question_count = original_text.count('?')\n",
    "    \n",
    "    return [num_words, uppercase_ratio, exclamation_count, question_count]\n",
    "\n",
    "print(\"Preprocessing text data...\")\n",
    "df['cleaned_text'] = df['text'].apply(advanced_clean_text)\n",
    "\n",
    "# Extract additional text features\n",
    "print(\"Extracting text features...\")\n",
    "text_features = np.array([extract_text_features(text) for text in df['text']])\n",
    "\n",
    "# Feature extraction: Create both word count and TF-IDF features\n",
    "print(\"Creating feature extraction pipelines...\")\n",
    "\n",
    "# Pipeline for TF-IDF features\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=3,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True,\n",
    "        use_idf=True,\n",
    "        max_features=40000\n",
    "    )),\n",
    "    ('selector', SelectKBest(score_func=chi2, k=25000))\n",
    "])\n",
    "\n",
    "# Pipeline for Count Vector features (binary)\n",
    "count_pipeline = Pipeline([\n",
    "    ('count', CountVectorizer(\n",
    "        ngram_range=(1, 4),\n",
    "        binary=True,\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )),\n",
    "    ('selector', SelectKBest(score_func=chi2, k=25000))\n",
    "])\n",
    "\n",
    "# Prepare data\n",
    "X_text = df['cleaned_text']\n",
    "y = df['hate']\n",
    "\n",
    "# Create stratified train/test split\n",
    "X_train_text, X_test_text, X_train_features, X_test_features, y_train, y_test = train_test_split(\n",
    "    X_text, text_features, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Transform text features using both pipelines\n",
    "print(\"Transforming features...\")\n",
    "X_train_tfidf = tfidf_pipeline.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf_pipeline.transform(X_test_text)\n",
    "\n",
    "X_train_count = count_pipeline.fit_transform(X_train_text)\n",
    "X_test_count = count_pipeline.transform(X_test_text)\n",
    "\n",
    "# Concatenate features from both vectorizers with additional text features\n",
    "from scipy.sparse import hstack\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_count, X_train_features])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_count, X_test_features])\n",
    "\n",
    "# Check for class imbalance\n",
    "class_distribution = np.bincount(y_train) / len(y_train)\n",
    "print(f\"Class distribution in training set: {class_distribution}\")\n",
    "\n",
    "# Apply SMOTE for mild oversampling of minority class if needed\n",
    "if abs(class_distribution[0] - class_distribution[1]) > 0.05:\n",
    "    print(\"Applying SMOTE to balance classes...\")\n",
    "    smote = SMOTE(random_state=42, sampling_strategy=0.95)  # not completely balanced\n",
    "    X_train_combined, y_train = smote.fit_resample(X_train_combined, y_train)\n",
    "    print(f\"New class distribution: {np.bincount(y_train) / len(y_train)}\")\n",
    "\n",
    "# Define individual models\n",
    "print(\"Training models...\")\n",
    "nb = MultinomialNB(alpha=0.05)  # Using your best alpha\n",
    "lr = LogisticRegression(C=5, max_iter=1000, class_weight='balanced', random_state=42)\n",
    "svm = LinearSVC(C=1, class_weight='balanced', dual=False, max_iter=10000, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=50, min_samples_leaf=2, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train individual models for comparison\n",
    "start_time = time.time()\n",
    "nb.fit(X_train_combined, y_train)\n",
    "y_pred_nb = nb.predict(X_test_combined)\n",
    "print(f\"\\nNaiveBayes - Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}, F1: {f1_score(y_test, y_pred_nb):.4f}\")\n",
    "\n",
    "lr.fit(X_train_combined, y_train)\n",
    "y_pred_lr = lr.predict(X_test_combined)\n",
    "print(f\"LogisticRegression - Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}, F1: {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "\n",
    "svm.fit(X_train_combined, y_train)\n",
    "y_pred_svm = svm.predict(X_test_combined)\n",
    "print(f\"SVM - Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}, F1: {f1_score(y_test, y_pred_svm):.4f}\")\n",
    "\n",
    "rf.fit(X_train_combined, y_train)\n",
    "y_pred_rf = rf.predict(X_test_combined)\n",
    "print(f\"RandomForest - Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}, F1: {f1_score(y_test, y_pred_rf):.4f}\")\n",
    "\n",
    "# Create voting ensemble with soft voting\n",
    "print(\"\\nTraining ensemble model...\")\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('nb', nb),\n",
    "        ('lr', lr),\n",
    "        ('svm', svm),\n",
    "        ('rf', rf)\n",
    "    ],\n",
    "    voting='soft',  # Use probabilities for better results\n",
    "    weights=[2, 1, 1, 1]  # Give more weight to NB since it performed well initially\n",
    ")\n",
    "\n",
    "# For soft voting with LinearSVC (which doesn't implement predict_proba)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "svm_calibrated = CalibratedClassifierCV(svm, cv=5)\n",
    "svm_calibrated.fit(X_train_combined, y_train)\n",
    "\n",
    "# Replace SVM with calibrated version for soft voting\n",
    "ensemble.estimators_[2] = ('svm', svm_calibrated)\n",
    "\n",
    "# Train ensemble\n",
    "ensemble.fit(X_train_combined, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate ensemble\n",
    "y_pred_ensemble = ensemble.predict(X_test_combined)\n",
    "accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "precision = precision_score(y_test, y_pred_ensemble)\n",
    "recall = recall_score(y_test, y_pred_ensemble)\n",
    "f1 = f1_score(y_test, y_pred_ensemble)\n",
    "\n",
    "print(f\"\\nEnsemble Model Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=['Not Hate', 'Hate']))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_ensemble))\n",
    "\n",
    "# 10-fold cross-validation on the best individual model (use the fastest one for CV)\n",
    "print(\"\\nPerforming cross-validation on NaiveBayes model...\")\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(nb, X_train_combined, y_train, cv=cv)\n",
    "print(f\"10-fold CV Accuracy: {cv_scores.mean():.4f}\")\n",
    "print(f\"95% CI: ({cv_scores.mean() - 2 * cv_scores.std():.4f}, {cv_scores.mean() + 2 * cv_scores.std():.4f})\")\n",
    "\n",
    "# Save the best model (either ensemble or individual based on performance)\n",
    "best_model = ensemble if accuracy_score(y_test, y_pred_ensemble) > accuracy_score(y_test, y_pred_nb) else nb\n",
    "print(f\"\\nSaving {'ensemble' if best_model == ensemble else 'NaiveBayes'} as best model...\")\n",
    "\n",
    "# Save necessary components for prediction\n",
    "model_data = {\n",
    "    'tfidf_pipeline': tfidf_pipeline,\n",
    "    'count_pipeline': count_pipeline,\n",
    "    'model': best_model,\n",
    "}\n",
    "joblib.dump(model_data, 'advanced_hate_speech_model.pkl')\n",
    "print(\"Model saved as 'advanced_hate_speech_model.pkl'\")\n",
    "\n",
    "# Prediction function for new texts\n",
    "def predict_hate_speech(text, model_data=model_data):\n",
    "    # Clean the text\n",
    "    cleaned_text = advanced_clean_text(text)\n",
    "    \n",
    "    # Extract additional features\n",
    "    text_features = extract_text_features(text)\n",
    "    \n",
    "    # Transform using both pipelines\n",
    "    tfidf_features = model_data['tfidf_pipeline'].transform([cleaned_text])\n",
    "    count_features = model_data['count_pipeline'].transform([cleaned_text])\n",
    "    \n",
    "    # Combine all features\n",
    "    combined_features = hstack([tfidf_features, count_features, [text_features]])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model_data['model'].predict(combined_features)[0]\n",
    "    \n",
    "    # Get probability if possible\n",
    "    try:\n",
    "        probability = model_data['model'].predict_proba(combined_features)[0][1]\n",
    "    except:\n",
    "        probability = None\n",
    "    \n",
    "    return {\n",
    "        'is_hate_speech': bool(prediction),\n",
    "        'probability': probability,\n",
    "        'cleaned_text': cleaned_text\n",
    "    }\n",
    "\n",
    "# Test the prediction function\n",
    "print(\"\\nExample classification:\")\n",
    "examples = [\n",
    "    \"I love how diverse and inclusive our community has become!\",\n",
    "    \"Those people are all the same, they should go back to where they came from\",\n",
    "    \"The weather is nice today, I might go for a walk\"\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    result = predict_hate_speech(example)\n",
    "    print(f\"\\nText: {example}\")\n",
    "    print(f\"Prediction: {'Hate Speech' if result['is_hate_speech'] else 'Not Hate Speech'}\")\n",
    "    if result['probability'] is not None:\n",
    "        print(f\"Probability: {result['probability']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
